#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Line Counting Web Module
Module ƒë·∫øm ng∆∞·ªùi qua ƒë∆∞·ªùng th·∫≥ng v·ªõi streaming web
"""

import cv2
import numpy as np
from ultralytics import YOLO
import torch
import torchvision.transforms as transforms
from torchvision import models
from scipy.spatial.distance import cdist
from scipy.optimize import linear_sum_assignment
import time
import base64
import threading
from collections import deque
import sys
import os

# Add paths for TensorRT support
current_dir = os.path.dirname(__file__)
core_dir = os.path.join(current_dir, '..', 'core')
sys.path.insert(0, core_dir)

try:
    from tensorrt_inference import create_yolo_model
    TENSORRT_AVAILABLE = True
except ImportError:
    TENSORRT_AVAILABLE = False
    create_yolo_model = None

# C·∫•u h√¨nh
CLASS_ID = 0  # ID cho class 'person'
CONF_THRESHOLD = 0.5

# CNN Feature Extractor
class FeatureExtractor:
    def __init__(self):
        self.model = models.resnet18(pretrained=True)
        self.model = torch.nn.Sequential(*list(self.model.children())[:-1])
        self.model.eval()
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((128, 64)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                 std=[0.229, 0.224, 0.225])
        ])
        if torch.cuda.is_available():
            self.model = self.model.cuda()

    def extract_features(self, image_patches):
        if len(image_patches) == 0:
            return np.array([])
        batch = []
        for patch in image_patches:
            if patch.size > 0:
                tensor = self.transform(patch)
                batch.append(tensor)
        if len(batch) == 0:
            return np.array([])
        batch = torch.stack(batch)
        if torch.cuda.is_available():
            batch = batch.cuda()
        with torch.no_grad():
            features = self.model(batch)
            features = features.squeeze()
            if len(features.shape) == 1:
                features = features.unsqueeze(0)
            features = features.cpu().numpy()
        return features

class Track:
    def __init__(self, track_id, detection, feature):
        self.track_id = track_id
        self.hits = 1
        self.time_since_update = 0
        self.feature = feature
        self.positions = []
        x1, y1, x2, y2 = detection[:4]
        self.x = (x1 + x2) / 2
        self.y = (y1 + y2) / 2
        self.positions.append((self.x, self.y))
        self.bbox = detection[:4]
        self.crossing_history = deque(maxlen=5)
        self.counted = False

    def update(self, detection, feature):
        self.hits += 1
        self.time_since_update = 0
        self.feature = 0.8 * self.feature + 0.2 * feature
        x1, y1, x2, y2 = detection[:4]
        self.x = (x1 + x2) / 2
        self.y = (y1 + y2) / 2
        self.positions.append((self.x, self.y))
        if len(self.positions) > 5:
            self.positions.pop(0)
        self.bbox = detection[:4]

    def predict(self):
        self.time_since_update += 1

class DeepTracker:
    def __init__(self, max_age=30, min_hits=3):
        self.max_age = max_age
        self.min_hits = min_hits
        self.tracks = []
        self.track_id_counter = 1
        self.feature_extractor = FeatureExtractor()
        # Kh√¥ng skip frames cho accuracy t·ªëi ƒëa

    def update(self, detections, frame):
        if len(detections) == 0:
            # Clean up old tracks
            self.tracks = [t for t in self.tracks if t.time_since_update < self.max_age]
            for track in self.tracks:
                track.predict()
            return []
        
        # Extract features cho m·ªçi detection
        patches = []
        for det in detections:
            x1, y1, x2, y2 = int(det[0]), int(det[1]), int(det[2]), int(det[3])
            patch = frame[y1:y2, x1:x2] 
            patches.append(patch)
        features = self.feature_extractor.extract_features(patches)
        cost_matrix = self._calculate_cost_matrix(detections, features)
        
        if len(self.tracks) > 0 and cost_matrix.size > 0:
            row_indices, col_indices = linear_sum_assignment(cost_matrix)
            matched_tracks = set()
            for row, col in zip(row_indices, col_indices):
                if cost_matrix[row, col] < 0.7:
                    self.tracks[row].update(detections[col], features[col])
                    matched_tracks.add(row)
            for i, det in enumerate(detections):
                if i not in col_indices or cost_matrix[row_indices[list(col_indices).index(i)], i] >= 0.7:
                    new_track = Track(self.track_id_counter, det, features[i])
                    self.tracks.append(new_track)
                    self.track_id_counter += 1
            self.tracks = [t for i, t in enumerate(self.tracks) 
                           if i in matched_tracks or t.time_since_update < self.max_age]
        else:
            for i, det in enumerate(detections):
                new_track = Track(self.track_id_counter, det, features[i])
                self.tracks.append(new_track)
                self.track_id_counter += 1
        return [t for t in self.tracks if t.hits >= self.min_hits]

    def _calculate_cost_matrix(self, detections, features):
        if len(self.tracks) == 0 or len(detections) == 0:
            return np.array([]).reshape(0, 0)
        track_features = np.array([t.feature for t in self.tracks])
        feature_distances = cdist(track_features, features, metric='cosine')
        track_positions = np.array([[t.x, t.y] for t in self.tracks])
        det_positions = np.array([[d[0] + d[2]/2, d[1] + d[3]/2] for d in detections])
        position_distances = cdist(track_positions, det_positions, metric='euclidean')
        position_distances = position_distances / np.max(position_distances) if np.max(position_distances) > 0 else position_distances
        return 0.7 * feature_distances + 0.3 * position_distances

class LineCounterWeb:
    def __init__(self, video_path, model_path, socketio, logger):
        self.video_path = video_path
        self.model_path = model_path
        self.socketio = socketio
        self.logger = logger
        
        # Kh·ªüi t·∫°o
        self.cap = None
        self.model = None
        self.tracker = None
        self.running = False
        
        # Line counting variables
        self.line_points = []
        self.line_ready = False
        self.counts = {'IN': 0, 'OUT': 0}
        
        # Auto setup line (gi·ªØa m√†n h√¨nh, ngang) - s·∫Ω b·ªã ghi ƒë√® khi user v·∫Ω
        self.auto_setup_line = True
        self.user_defined_line = False
        
    def initialize(self):
        """Kh·ªüi t·∫°o camera/video v√† model"""
        try:
            # M·ªü video source
            if isinstance(self.video_path, int):
                self.logger.info(f'M·ªü camera ID: {self.video_path}')
                self.cap = cv2.VideoCapture(self.video_path)
            else:
                self.logger.info(f'M·ªü video file: {self.video_path}')
                self.cap = cv2.VideoCapture(self.video_path)
            
            if not self.cap.isOpened():
                raise Exception(f"Kh√¥ng th·ªÉ m·ªü video source: {self.video_path}")
            
            # Load YOLO model with TensorRT support
            self.logger.info(f'Loading model: {self.model_path}')
            
            if TENSORRT_AVAILABLE and self.model_path.endswith('.engine'):
                self.model = create_yolo_model('models/' + self.model_path, self.logger)
                self.logger.info(f'üöÄ TensorRT engine loaded: {self.model_path}')
            else:
                self.model = YOLO('models/' + self.model_path)
                self.logger.info(f'üì¶ PyTorch model loaded: {self.model_path}')
            
            # Kh·ªüi t·∫°o tracker
            self.tracker = DeepTracker(max_age=30, min_hits=3)
            
            # Auto setup line n·∫øu c·∫ßn v√† ch∆∞a c√≥ user-defined line
            if self.auto_setup_line and not self.user_defined_line:
                ret, frame = self.cap.read()
                if ret:
                    h, w = frame.shape[:2]
                    # T·∫°o ƒë∆∞·ªùng ngang ·ªü gi·ªØa m√†n h√¨nh
                    self.line_points = [
                        (w // 4, h // 2),
                        (3 * w // 4, h // 2)
                    ]
                    self.line_ready = True
                    self.logger.info(f'T·ª± ƒë·ªông t·∫°o ƒë∆∞·ªùng ƒë·∫øm: {self.line_points}')
                    # Reset l·∫°i video
                    self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
            
            return True
            
        except Exception as e:
            self.logger.error(f'L·ªói kh·ªüi t·∫°o: {str(e)}')
            return False
    
    def run(self):
        """Ch·∫°y line counting"""
        if not self.initialize():
            return
        
        self.running = True
        self.logger.success('B·∫Øt ƒë·∫ßu Line Counting')
        
        frame_count = 0
        fps_time = time.time()
        
        try:
            while self.running:
                ret, frame = self.cap.read()
                if not ret:
                    if isinstance(self.video_path, str):  # Video file ended
                        self.logger.info('Video ƒë√£ k·∫øt th√∫c, l·∫∑p l·∫°i...')
                        self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                        continue
                    else:  # Camera error
                        self.logger.error('L·ªói ƒë·ªçc frame t·ª´ camera')
                        break
                
                frame_count += 1
                
                # V·∫Ω ƒë∆∞·ªùng ƒë·∫øm
                self.draw_line(frame)
                
                # YOLO detection - ch·∫°y tr√™n m·ªçi frame
                results = self.model(frame, classes=[CLASS_ID], conf=CONF_THRESHOLD, verbose=False)
                detections = []
                
                if len(results[0].boxes) > 0:
                    boxes = results[0].boxes.xyxy.cpu().numpy()
                    confidences = results[0].boxes.conf.cpu().numpy()
                    for box, conf in zip(boxes, confidences):
                        x1, y1, x2, y2 = box
                        detections.append([x1, y1, x2, y2, conf])
                
                # Tracking
                active_tracks = self.tracker.update(detections, frame)
                
                # Ki·ªÉm tra crossing v√† v·∫Ω tracking
                for track in active_tracks:
                    direction = self.check_crossing(track)
                    x1, y1, x2, y2 = track.bbox
                    
                    # M√†u box d·ª±a tr√™n crossing
                    color = (0, 255, 255) if direction else (255, 0, 0)
                    cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)
                    cv2.circle(frame, (int(track.x), int(track.y)), 5, color, -1)
                    
                    # Label
                    label = f"ID:{track.track_id}"
                    if direction:
                        label += f" {direction}"
                    cv2.putText(frame, label, (int(x1), int(y1)-10), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
                    
                    # V·∫Ω trajectory
                    if len(track.positions) > 1:
                        for i in range(1, len(track.positions)):
                            pt1 = (int(track.positions[i-1][0]), int(track.positions[i-1][1]))
                            pt2 = (int(track.positions[i][0]), int(track.positions[i][1]))
                            cv2.line(frame, pt1, pt2, (128, 128, 128), 2)
                
                # V·∫Ω stats tr√™n frame
                self.draw_stats(frame)
                
                # G·ª≠i frame qua Socket.IO
                self.send_frame(frame)
                
                # G·ª≠i stats update
                self.send_stats()
                
                # FPS logging v·ªõi th√¥ng tin TensorRT
                if frame_count % 60 == 0:
                    current_time = time.time()
                    fps = 60 / (current_time - fps_time) if current_time > fps_time else 0
                    fps_time = current_time
                    model_type = "üöÄ TensorRT" if self.model_path.endswith('.engine') else "üì¶ PyTorch"
                    self.logger.info(f'{model_type} FPS: {fps:.1f} | Tracks: {len(active_tracks)}')
                
                # T·ªëi ∆∞u t·ªëc ƒë·ªô streaming - gi·∫£m delay
                time.sleep(0.01)  # ~60 FPS target
                
        except Exception as e:
            self.logger.error(f'L·ªói trong qu√° tr√¨nh counting: {str(e)}')
        finally:
            self.cleanup()
    
    def check_crossing(self, track):
        """Ki·ªÉm tra crossing qua line"""
        if track.counted or not self.line_ready or len(self.line_points) != 2 or len(track.positions) < 2:
            return None
        
        prev = track.positions[-2]
        curr = track.positions[-1]
        
        # Ki·ªÉm tra line intersection
        intersects, _ = self.line_intersection(prev, curr, self.line_points[0], self.line_points[1])
        
        if intersects:
            direction = self.get_crossing_direction(prev, curr, self.line_points[0], self.line_points[1])
            track.crossing_history.append(direction)
        else:
            track.crossing_history.append(None)
        
        # X√°c ƒë·ªãnh crossing
        history = list(track.crossing_history)
        if history.count("IN") >= 1:
            self.counts["IN"] += 1
            track.counted = True
            self.logger.success(f'Ng∆∞·ªùi #{track.track_id} v√†o | IN = {self.counts["IN"]}')
            return "IN"
        elif history.count("OUT") >= 1:
            self.counts["OUT"] += 1
            track.counted = True
            self.logger.success(f'Ng∆∞·ªùi #{track.track_id} ra | OUT = {self.counts["OUT"]}')
            return "OUT"
        
        return None
    
    def line_intersection(self, p1, p2, q1, q2):
        """Ki·ªÉm tra intersection gi·ªØa 2 ƒë∆∞·ªùng th·∫≥ng"""
        x1, y1 = p1
        x2, y2 = p2
        x3, y3 = q1
        x4, y4 = q2
        
        denom = (x1-x2)*(y3-y4) - (y1-y2)*(x3-x4)
        if abs(denom) < 1e-10:
            return False, None
        
        t = ((x1-x3)*(y3-y4) - (y1-y3)*(x3-x4)) / denom
        u = -((x1-x2)*(y1-y3) - (y1-y2)*(x1-x3)) / denom
        
        if 0 <= t <= 1 and 0 <= u <= 1:
            ix = x1 + t*(x2 - x1)
            iy = y1 + t*(y2 - y1)
            return True, (ix, iy)
        
        return False, None
    
    def get_crossing_direction(self, prev, curr, line_p1, line_p2):
        """X√°c ƒë·ªãnh h∆∞·ªõng crossing"""
        dx, dy = curr[0] - prev[0], curr[1] - prev[1]
        lx, ly = line_p2[0] - line_p1[0], line_p2[1] - line_p1[1]
        cross = dx * ly - dy * lx
        return "IN" if cross > 0 else "OUT"
    
    def draw_line(self, frame):
        """V·∫Ω ƒë∆∞·ªùng ƒë·∫øm"""
        if len(self.line_points) == 2:
            cv2.line(frame, self.line_points[0], self.line_points[1], (0, 0, 255), 3)
            cv2.circle(frame, self.line_points[0], 8, (0, 255, 0), -1)
            cv2.circle(frame, self.line_points[1], 8, (0, 255, 0), -1)
            
            # V·∫Ω label IN/OUT
            mid_x = (self.line_points[0][0] + self.line_points[1][0]) // 2
            mid_y = (self.line_points[0][1] + self.line_points[1][1]) // 2
            cv2.putText(frame, "IN", (mid_x - 50, mid_y - 20), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            cv2.putText(frame, "OUT", (mid_x + 20, mid_y + 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
    
    def draw_stats(self, frame):
        """V·∫Ω th·ªëng k√™ tr√™n frame"""
        # Background box
        cv2.rectangle(frame, (10, 10), (300, 100), (0, 0, 0), -1)
        cv2.rectangle(frame, (10, 10), (300, 100), (255, 255, 255), 2)
        
        # Text
        cv2.putText(frame, "LINE COUNTING", (20, 35), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        cv2.putText(frame, f"IN:  {self.counts['IN']}", (20, 60), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        cv2.putText(frame, f"OUT: {self.counts['OUT']}", (20, 85), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        
        net = self.counts['IN'] - self.counts['OUT']
        cv2.putText(frame, f"NET: {net}", (150, 60), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
    
    def send_frame(self, frame):
        """G·ª≠i frame qua Socket.IO v·ªõi metadata v·ªÅ k√≠ch th∆∞·ªõc - T·ªëi ∆∞u h√≥a"""
        try:
            # L∆∞u k√≠ch th∆∞·ªõc g·ªëc
            original_height, original_width = frame.shape[:2]
            
            # Aggressive resize ƒë·ªÉ tƒÉng t·ªëc
            display_frame = frame.copy()
            scale_factor = 1.0
            
            # Resize xu·ªëng k√≠ch th∆∞·ªõc nh·ªè h∆°n ƒë·ªÉ tƒÉng t·ªëc
            if original_width > 640:  # Gi·∫£m t·ª´ 800 xu·ªëng 640
                scale_factor = 640 / original_width
                new_width = int(original_width * scale_factor)
                new_height = int(original_height * scale_factor)
                display_frame = cv2.resize(display_frame, (new_width, new_height), interpolation=cv2.INTER_LINEAR)
            
            # T·ªëi ∆∞u JPEG compression
            encode_params = [
                cv2.IMWRITE_JPEG_QUALITY, 60,  # Gi·∫£m t·ª´ 70 xu·ªëng 60
                cv2.IMWRITE_JPEG_OPTIMIZE, 1,
                cv2.IMWRITE_JPEG_PROGRESSIVE, 1
            ]
            ret, buffer = cv2.imencode('.jpg', display_frame, encode_params)
            
            if ret:
                frame_bytes = base64.b64encode(buffer).decode('utf-8')
                
                # G·ª≠i v·ªõi metadata
                frame_data = {
                    'image': frame_bytes,
                    'original_size': {
                        'width': original_width,
                        'height': original_height
                    },
                    'display_size': {
                        'width': display_frame.shape[1],
                        'height': display_frame.shape[0]
                    },
                    'scale_factor': scale_factor
                }
                self.socketio.emit('video_frame', frame_data)
        except Exception as e:
            print(f"Send frame error: {e}")
    
    def send_stats(self):
        """G·ª≠i stats qua Socket.IO"""
        try:
            self.socketio.emit('stats_update', {
                'in_count': self.counts['IN'],
                'out_count': self.counts['OUT'],
                'zone_count': 0  # Line counting kh√¥ng c√≥ zone count
            })
        except Exception as e:
            print(f"Send stats error: {e}")
    
    def stop(self):
        """D·ª´ng counting"""
        self.running = False
        # Cleanup TensorRT resources
        if hasattr(self.model, 'cleanup'):
            self.model.cleanup()
        self.logger.info('Line Counting ƒë√£ d·ª´ng')
    
    def reset(self):
        """Reset counting"""
        self.counts = {'IN': 0, 'OUT': 0}
        if self.tracker:
            self.tracker = DeepTracker(max_age=30, min_hits=3)
        self.logger.info('ƒê√£ reset Line Counting')
    
    def cleanup(self):
        """D·ªçn d·∫πp resources"""
        if self.cap:
            self.cap.release()
        # Cleanup TensorRT resources
        if hasattr(self.model, 'cleanup'):
            self.model.cleanup()
        self.logger.info('ƒê√£ d·ªçn d·∫πp resources')
    
    def set_line_points(self, points):
        """Thi·∫øt l·∫≠p ƒëi·ªÉm line t·ª´ web interface"""
        if len(points) >= 2:
            self.line_points = points[:2]
            self.line_ready = True
            self.user_defined_line = True
            self.logger.info(f'ƒê√£ thi·∫øt l·∫≠p line points: {self.line_points}')
    
    def clear_drawing(self):
        """X√≥a drawing"""
        self.line_points = []
        self.line_ready = False
        self.user_defined_line = False
        self.logger.info('ƒê√£ x√≥a line drawing')

if __name__ == "__main__":
    # Test standalone
    class DummySocketIO:
        def emit(self, event, data):
            print(f"Emit {event}: {data}")
    
    class DummyLogger:
        def info(self, msg): print(f"INFO: {msg}")
        def success(self, msg): print(f"SUCCESS: {msg}")
        def error(self, msg): print(f"ERROR: {msg}")
    
    counter = LineCounterWeb('Test.mp4', 'yolov8s.pt', DummySocketIO(), DummyLogger())
    counter.run()
